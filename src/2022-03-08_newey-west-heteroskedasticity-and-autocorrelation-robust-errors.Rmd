---
title: "Modeling autocorrelations: Newey-West HAC and ARIMA modeling"
author: "Nayef Ahmad"
date: "2022-03-08" 
output: 
   github_document: 
     toc: true
     number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview 

When there is autocorrelation of residuals, this means that your model is leaving value on the table - out of sheer laziness, it has not accounted for all of the structure available. You should not want to listen to the advice of someone who does not work as hard as you do. Ask them to work harder and come back to you after they've done their homework. Then you can discuss results (i.e. make inferences). 


In this file, we explore two ways to get your model to "work harder" and account for autocorrelation. The first is to use OLS as usual, then correct the estimate of the covariance matrix of the parameters. The second is to directly model autocorrelation using an ARIMA model. 

**References:** 

- [Cross Validated - OLS regression with Newey-West error term](https://stats.stackexchange.com/a/254596/56828). This includes a simulation of autocorrelated residuals, and compares ordinary OLS vs HAC inferences. 
- [Cross Validated - two ways of dealing with the problem of autocorrelated errors](https://stats.stackexchange.com/a/181297/56828). This argues that the ARIMA approach is better than using OLS + HAC errors. 


# Libraries 

```{r}
library(lmtest)
library(sandwich)
library(forecast)
library(tidyverse)

par(mfrow = c(1,3))

layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)
# layout.show(3)

```

# Functions 

```{r}
data_with_white_noise_error <- function(iterations = 20, 
                                        n = 50){
  # Create a dataframe of simulated X and Y data, for a given sample size.
  # Each iteration is a single sample of size n for the given parameters.  
  # We want several samples for each set of parameters. 
  # 
  # Args:
  #   - iterations: number of datasets to create 
  # 
  # Returns: 
  #   - df: dataframe with columns x and y
  
  slope <- .05 
  
  white_noise_residuals <- rnorm(n * iterations)
  x <- rep(seq(n), iterations)
  y <- slope * x + white_noise_residuals
    
  sample_id <- rep(seq(1:iterations), each = n)
    
  
  df <- data.frame(sample_id, x , y)
  
  return(df)
  
}






get_slope_line_vector_from_arima <- function(arima_model_fitted){
  # Args: 
  # - arima_model_fitted: Fitted ARIMA model, such as the models returned 
  #     by forecast::auto.arima()
  # 
  # Returns: 
  # - line_vector: A numeric vector of same length as the data passed to 
  #     the ARIMA fitting procedure. These values represent the underlying 
  #     linear trend after fitting the ARIMA model 
  
  if (is.na(arima_model_fitted$coef["intercept"])) {
    intercept <- 0
  } else {
    intercept <- arima_model_fitted$coef["intercept"]
  }
  
  if (is.na(arima_model_fitted$coef["xreg"])) {
    slope <- 0
  } else {
    slope <- arima_model_fitted$coef["xreg"]
  }
  
  line_vector <- 
    intercept + slope * c(1:arima_model_fitted$nobs)
  
  return(line_vector)
}
```


# Data generation 

Let's visualize what these series look like, for a given n value. 


## White noise error 

```{r}
seed <- 2022
set.seed(seed)

df <- data_with_white_noise_error(iterations = 20)

df %>% 
  ggplot(aes(x = x, y = y, group = sample_id)) + 
  geom_line(alpha=.5) + 
  facet_wrap(~sample_id) + 
  theme_minimal()

```


Now let's collect several different sample sizes into dataframes. 

```{r}


# df_white_noise <- multiple_n_white_noise_error(iterations = i, n_values = c(50, 100)


# multiple_n_white_noise_error: 
#   - for loops over n_values, calls data_with_white_noise_error()
#   - collects all series, does group_by(n, iteration_num, ar_coef) %>% cur_group_id() 



# df_arima_error <- data_with_arima_error(iterations = i, n = 50, ar_coeffecients = seq(0, .9, .2))

# df_arima <- data_with_arima_response(iterations = i, n = 50, ar_coeffecients = seq(0, .9, .2))



```




# Example 1: White noise error (base case)

First set up the data.

```{r}
n <- 50
slope <- .05


white_noise_residuals <- rnorm(n)
x <- 1:n
y <- slope*(x) + white_noise_residuals

```


Fit lm model, and plot. 

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)


fit <- lm(y~x)

plot(x,y)
abline(fit, col = 'blue')
acf(fit$residuals)
pacf(fit$residuals)

```

Model output, using standard and Newey-West HAC errors: 

```{r}
summary(fit) # standard estimates
coeftest(fit, vcov = NeweyWest(fit, verbose = T))

```


Fit ARIMA model, and plot

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)

fit_arima <- auto.arima(y, xreg = x)

plot(x,y)
lines(fit_arima$fitted, col = "blue")
lines(get_slope_line_vector_from_arima(fit_arima), col = "darkgreen")
acf(fit_arima$residuals)
pacf(fit_arima$residuals)

```

ARIMA model output: 

```{r}
summary(fit_arima)
```


# Example 2: Autocorrelated error 

In cases where non-arima fit gives almost same estimate as arima fit, but p-value of t-test for coefficient is far from significant, I would prefer the arima fit. 

- Examples where Arima fit is better than OLS:
  - seed 1, 2, 4, 5, 10, 13, 16, 17, 20 
- Examples where Arima fit is better than OLS + HAC:
  - seed 1, 2, 4, 5, 10, 12, 13, 14, 15, 16, 17, 19, 20 
- Examples where no benefit to Arima:
  - seed 3, 6, 7, 8, 9, 11, 18 

First set up the data.

```{r}
seed <- 2
set.seed(seed)

n <- 50
slope <- .05


correlated_residuals <- arima.sim(list(ar = .9), n)
x <- 1:n
y <- slope*(x) + correlated_residuals

```

Fit lm model, and plot. 

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)

fit <- lm(y~x)

plot(x,y)
abline(fit, col = 'blue')
acf(fit$residuals)
pacf(fit$residuals)

```

Model output, using standard and Newey-West HAC errors: 

```{r}
summary(fit) # standard estimates
coeftest(fit, vcov = NeweyWest(fit, verbose = T))

```

Fit ARIMA model, and plot

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)

fit_arima <- auto.arima(y, xreg = x)
summary(fit_arima)

plot(x,y)
lines(fit_arima$fitted, col = "blue")
lines(get_slope_line_vector_from_arima(fit_arima), col = "darkgreen")
acf(fit_arima$residuals)
pacf(fit_arima$residuals)

```

In this specific example, where `seed = ` `r seed`, we can see that the OLS fit and the (OLS + HAC) fit find the estimate a slope value close to the true value of `r slope`, but because of the large amount of unexplained structure, inference is not valid - the slope coefficient is not significant. 

On the other hand, the ARIMA fit includes the slope, and the estimate is relatively close to the true value. 

# Example 3: Pure ARIMA series as dependent variable 

First set up the data.

```{r}
seed <- 3
set.seed(seed)

n <- 50
slope <- .05


y <- arima.sim(list(ar = .9), n)
x <- 1:n

```

Fit lm model, and plot. 

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)

fit <- lm(y~x)

plot(x,y)
abline(fit, col = 'blue')
acf(fit$residuals)
pacf(fit$residuals)

```

Model output, using standard and Newey-West HAC errors: 

```{r}

summary(fit) # standard estimates
coeftest(fit, vcov = NeweyWest(fit, verbose = T))
```

Fit ARIMA model, and plot

```{r}
layout.matrix <- matrix(c(1, 2, 1, 3), nrow = 2, ncol = 2)
layout(mat = layout.matrix)

fit_arima <- auto.arima(y, xreg = x)
summary(fit_arima)

plot(x,y)
lines(fit_arima$fitted, col = "blue")
lines(get_slope_line_vector_from_arima(fit_arima), col = "darkgreen")
acf(fit_arima$residuals)
pacf(fit_arima$residuals)
```

